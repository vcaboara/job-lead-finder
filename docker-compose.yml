services:
  app:
    build:
      context: .
      args:
        INSTALL_TEST_DEPS: "true"
        INSTALL_GEMINI: "true"
    volumes:
      - ./:/app:delegated
    working_dir: /app
    environment:
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - RAPIDAPI_KEY=${RAPIDAPI_KEY}
    # Keep container running for interactive execs; use `docker compose run` for one-off commands.
    command: tail -f /dev/null
    tty: true

  ui:
    extends: app
    volumes:
      - ./:/app:delegated
      - job-data:/app/data # Shared volume for database
      - job-logs:/app/logs # Shared volume for logs
    environment:
      - GITHUB_TOKEN=${GITHUB_TOKEN}
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
    ports:
      - "8000:8000"
    command: python -m uvicorn app.ui_server:app --host 0.0.0.0 --port 8000
    tty: true

  worker:
    extends: app
    volumes:
      - ./:/app:delegated
      - job-data:/app/data # Shared volume for database
      - job-logs:/app/logs # Shared volume for logs
    working_dir: /app
    environment:
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - RAPIDAPI_KEY=${RAPIDAPI_KEY}
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
    command: python -m app.worker
    restart: unless-stopped
    depends_on:
      - ui

  ai-monitor:
    extends: app
    volumes:
      - ./:/app:delegated
      - ai-tracking:/app/.ai_usage_tracking/ # Persistent tracking directory for JSON files
    working_dir: /app
    environment:
      - AI_TRACKING_FILE=/app/.ai_usage_tracking/.ai_usage_tracking.json
    ports:
      - "9000:9000"
    command: python -m app.ai_monitor_ui
    restart: unless-stopped
    depends_on:
      - ui

  pr-monitor:
    build:
      context: .
      dockerfile: Dockerfile.pr-monitor
    environment:
      - GITHUB_TOKEN=${GITHUB_TOKEN}
      - REPO_OWNER=vcaboara
      - REPO_NAME=job-lead-finder
      - CHECK_INTERVAL=${PR_CHECK_INTERVAL:-300} # 5 minutes default
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  vibe-check-mcp:
    build:
      context: ./vibe-check-mcp
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - GOOGLE_CLOUD_PROJECT=${GOOGLE_CLOUD_PROJECT:-mcp-vibetest}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-gemini-2.5-flash}
      - DEFAULT_LLM_PROVIDER=${DEFAULT_LLM_PROVIDER:-ollama}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-deepseek-coder:6.7b}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - USE_LEARNING_HISTORY=${USE_LEARNING_HISTORY:-false}
    restart: unless-stopped
    # Volume mount removed - built files are in the image
    # If you need dev mode with hot reload, use docker-compose.dev.yml override

  brave-search-mcp:
    # NOTE: Official MCP servers use stdio (not HTTP ports)
    # This service is for use with 'docker compose run' commands
    # See docs/MCP_SERVICES_GUIDE.md for proper configuration
    image: node:20-alpine
    profiles: ["mcp"]  # Don't start automatically with 'docker compose up'
    environment:
      - BRAVE_API_KEY=${BRAVE_API_KEY}
    working_dir: /app
    command: sh -c "npx -y @modelcontextprotocol/server-brave-search"
    stdin_open: true
    tty: true

  fetch-mcp:
    # NOTE: Official MCP servers use stdio (not HTTP ports)
    # This service is for use with 'docker compose run' commands
    # See docs/MCP_SERVICES_GUIDE.md for proper configuration
    image: node:20-alpine
    profiles: ["mcp"]  # Don't start automatically with 'docker compose up'
    working_dir: /app
    command: sh -c "npx -y @modelcontextprotocol/server-fetch"
    stdin_open: true
    tty: true

  ollama-tunnel:
    image: cloudflare/cloudflared:latest
    command: tunnel --no-autoupdate --url http://host.docker.internal:11434
    restart: unless-stopped
    # Logs will show the public HTTPS URL for GitHub Actions
    # Run: docker compose logs ollama-tunnel | grep "https://"
    # Then: gh secret set OLLAMA_BASE_URL --body "https://your-url.trycloudflare.com"

volumes:
  job-data:
  job-logs:
  ai-tracking:
