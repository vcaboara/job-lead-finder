services:
  app:
    build:
      context: .
      args:
        INSTALL_TEST_DEPS: "true"
        INSTALL_GEMINI: "true"
    volumes:
      - ./:/app:delegated
    working_dir: /app
    environment:
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - RAPIDAPI_KEY=${RAPIDAPI_KEY}
    # Keep container running for interactive execs; use `docker compose run` for one-off commands.
    command: tail -f /dev/null
    tty: true

  ui:
    extends: app
    volumes:
      - ./:/app:delegated
      - job-data:/app/data # Shared volume for database
      - job-logs:/app/logs # Shared volume for logs
    environment:
      - GITHUB_TOKEN=${GITHUB_TOKEN}
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
    ports:
      - "8000:8000"
    command: python -m uvicorn app.ui_server:app --host 0.0.0.0 --port 8000
    tty: true

  worker:
    extends: app
    volumes:
      - ./:/app:delegated
      - job-data:/app/data # Shared volume for database
      - job-logs:/app/logs # Shared volume for logs
    working_dir: /app
    environment:
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - RAPIDAPI_KEY=${RAPIDAPI_KEY}
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
    command: python -m app.worker
    restart: unless-stopped
    depends_on:
      - ui

  ai-monitor:
    extends: app
    volumes:
      - ./:/app:delegated
      - ai-tracking:/app/.ai_usage_tracking/ # Persistent tracking directory for JSON files
    working_dir: /app
    environment:
      - AI_TRACKING_FILE=/app/.ai_usage_tracking/.ai_usage_tracking.json
    ports:
      - "9000:9000"
    command: python -m app.ai_monitor_ui
    restart: unless-stopped
    depends_on:
      - ui

  vibe-check-mcp:
    build:
      context: ./vibe-check-mcp
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      - GOOGLE_CLOUD_PROJECT=${GOOGLE_CLOUD_PROJECT:-mcp-vibetest}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-gemini-2.5-flash}
      - DEFAULT_LLM_PROVIDER=${DEFAULT_LLM_PROVIDER:-ollama}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-deepseek-coder:6.7b}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - USE_LEARNING_HISTORY=${USE_LEARNING_HISTORY:-false}
    restart: unless-stopped
    # Volume mount removed - built files are in the image
    # If you need dev mode with hot reload, use docker-compose.dev.yml override

  brave-search-mcp:
    image: node:20-alpine
    ports:
      - "3002:3002"
    environment:
      - BRAVE_API_KEY=${BRAVE_API_KEY}
      - PORT=3002
    working_dir: /app
    command: sh -c "npx -y @modelcontextprotocol/server-brave-search"
    restart: unless-stopped

  fetch-mcp:
    image: node:20-alpine
    ports:
      - "3003:3003"
    environment:
      - PORT=3003
    working_dir: /app
    command: sh -c "npx -y @modelcontextprotocol/server-fetch"
    restart: unless-stopped

  ollama-tunnel:
    image: cloudflare/cloudflared:latest
    command: tunnel --no-autoupdate --url http://host.docker.internal:11434
    restart: unless-stopped
    # Logs will show the public HTTPS URL for GitHub Actions
    # Run: docker compose logs ollama-tunnel | grep "https://"
    # Then: gh secret set OLLAMA_BASE_URL --body "https://your-url.trycloudflare.com"

volumes:
  job-data:
  job-logs:
  ai-tracking:
